Statistics with Python-Frequency Distributions in Statistics.

- [

[298f76_0e1050bccc614725bb5005e3a5ab5a54~mv2.webp](../_resources/b4641098fec648695984df33e0e9aafd.webp)](https://twitter.com/b_prasad26)

- [

[48a2a42b19814efaa824450f23e8a253.webp](../_resources/e4c5cb65665a0a195b0ce22ce190d3e3.webp)](https://www.facebook.com/lifewithdata26/)

- [

[298f76_35a0a6ce4040440b80604cdd1a7d6801~mv2.webp](../_resources/adcc1768a5640c2d0abb2d65b779065b.webp)](https://www.linkedin.com/in/bhola-prasad-0065834b/)

- [

[062430dbfeba4663a6bf9465b05dee18.webp](../_resources/15f3645d4103eb1c1f3a1a353e14560b.webp)](http://feeds.feedburner.com/LifeWithData)

- [ Blog](https://www.lifewithdata.com/?utm_medium=Community&utm_source=DataCamp.com)
- [ Start Here](https://www.lifewithdata.com/start-here?utm_medium=Community&utm_source=DataCamp.com)
- [ About Me](https://www.lifewithdata.com/about-me?utm_medium=Community&utm_source=DataCamp.com)
- [ Contact](https://www.lifewithdata.com/contact?utm_medium=Community&utm_source=DataCamp.com)
- [ Members](https://www.lifewithdata.com/members?utm_medium=Community&utm_source=DataCamp.com)
- [ More]()

[9c4b521dd2404cd5a05ed6115f3a0dc8.webp](../_resources/0fd85b025a587d8c215855dbad956f64.webp)

# [Life With Data ](https://www.lifewithdata.com/?utm_medium=Community&utm_source=DataCamp.com)

### [Business Analytics & Data Science](https://www.lifewithdata.com/?utm_medium=Community&utm_source=DataCamp.com)

[

[20adf4_f22d72b63ad145c3b9f7bba1606ab376.webp](../_resources/d26d13182d37ad9caccf45aa80dd77d9.webp)](https://www.lifewithdata.com/?utm_medium=Community&utm_source=DataCamp.com)

[ ![](data:image/svg+xml,%3csvg viewBox='0 0 569 546' version='1.1' xmlns='http://www.w3.org/2000/svg' data-evernote-id='7' class='js-evernote-checked'%3e%3cg%3e%3ccircle data-color='1' id='Oval' cx='362.589996' cy='204.589996' r='204.589996' data-evernote-id='8' class='js-evernote-checked'%3e%3c/circle%3e%3crect data-color='2' id='Rectangle' x='0' y='0' width='100' height='545.799988' data-evernote-id='9' class='js-evernote-checked'%3e%3c/rect%3e%3c/g%3e%3c/svg%3e)  Become a patron](https://www.patreon.com/bePatron?u=12171782&redirect_uri=https%3A%2F%2Fwww.lifewithdata.com%2F&utm_medium=widget&utm_source=wix)

- [All Posts](https://www.lifewithdata.com/blog-1/)
- [Data Science](https://www.lifewithdata.com/blog-1/categories/data-science)
- [Python](https://www.lifewithdata.com/blog-1/categories/python)
- [Statistics](https://www.lifewithdata.com/blog-1/categories/statistics)
- [Google Analytics](https://www.lifewithdata.com/blog-1/categories/google-analytics)
- [PPC & SEO](https://www.lifewithdata.com/blog-1/categories/ppc-seo)
- [Excel](https://www.lifewithdata.com/blog-1/categories/excel)

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' width='19' viewBox='0 0 19 19' role='img' class='_1lTvN blog-desktop-header-search-icon-fill js-evernote-checked' data-evernote-id='478'%3e%3cpath d='M12.8617648%2c11.8617648 L15.8633394%2c14.8633394 C15.9414442%2c14.9414442 15.9414442%2c15.0680772 15.8633394%2c15.1461821 L15.1461821%2c15.8633394 C15.0680772%2c15.9414442 14.9414442%2c15.9414442 14.8633394%2c15.8633394 L11.8617648%2c12.8617648 C10.9329713%2c13.578444 9.76865182%2c14.0047607 8.50476074%2c14.0047607 C5.46719462%2c14.0047607 3.00476074%2c11.5423269 3.00476074%2c8.50476074 C3.00476074%2c5.46719462 5.46719462%2c3.00476074 8.50476074%2c3.00476074 C11.5423269%2c3.00476074 14.0047607%2c5.46719462 14.0047607%2c8.50476074 C14.0047607%2c9.76865182 13.578444%2c10.9329713 12.8617648%2c11.8617648 Z M8.5%2c13 C10.9852814%2c13 13%2c10.9852814 13%2c8.5 C13%2c6.01471863 10.9852814%2c4 8.5%2c4 C6.01471863%2c4 4%2c6.01471863 4%2c8.5 C4%2c10.9852814 6.01471863%2c13 8.5%2c13 Z' data-evernote-id='479' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

- [ Bhola Prasad![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' width='19' viewBox='0 0 19 19' style='fill-rule:evenodd' class='blog-icon-fill js-evernote-checked' data-evernote-id='482'%3e%3cpath d='M15.3812%2c6.495914 L12.6789333%2c8.77258837 C12.6191333%2c8.84477644 12.5099333%2c8.85722265 12.4354%2c8.79997005 C12.4215333%2c8.79001308 12.4094%2c8.77756686 12.3998667%2c8.76429089 L9.78686667%2c6.14327115 C9.67766667%2c5.99225704 9.46186667%2c5.95491839 9.305%2c6.05863687 C9.26946667%2c6.08186981 9.23913333%2c6.11091099 9.21573333%2c6.14493065 L6.60013333%2c8.81075677 C6.5464%2c8.88626383 6.43893333%2c8.90534803 6.3592%2c8.85390366 C6.34446667%2c8.84394669 6.33146667%2c8.83233022 6.32106667%2c8.81905425 L3.61966667%2c6.50587098 C3.5018%2c6.36149485 3.28426667%2c6.33577266 3.13346667%2c6.44861837 C3.0494%2c6.51167921 3%2c6.60792997 3%2c6.70998895 L4%2c14 L15%2c14 L16%2c6.70169148 C16%2c6.51831719 15.8448667%2c6.36979232 15.6533333%2c6.36979232 C15.5476%2c6.36979232 15.4470667%2c6.41625821 15.3812%2c6.495914 Z' data-evernote-id='483' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)](https://social-blog.wix.com/statistics-with-python-frequency-distributions-in-statistics?cacheKiller=1550010839858&compId=TPASection_jaavxmq5&deviceType=desktop&height=1864&instance=mb5S3pAb4xSSWk8ZvtIMzGAySbwSz3xPL3hOSsJlroo.eyJpbnN0YW5jZUlkIjoiNmJjYmNlZDYtNDgwYS00ZTI0LWExZDgtMjlkYWZmY2Q5MDI5IiwiYXBwRGVmSWQiOiIxNGJjZGVkNy0wMDY2LTdjMzUtMTRkNy00NjZjYjNmMDkxMDMiLCJtZXRhU2l0ZUlkIjoiZTY0YWEwNzgtOGQ1MC00YWI2LTk4ODItYmUxMDExM2M1MTk1Iiwic2lnbkRhdGUiOiIyMDE5LTAyLTEzVDA0OjI5OjM5LjU4N1oiLCJ1aWQiOm51bGwsImlwQW5kUG9ydCI6IjY3LjE4MC44Ni4xMTcvNTQ3MzYiLCJ2ZW5kb3JQcm9kdWN0SWQiOm51bGwsImRlbW9Nb2RlIjpmYWxzZSwib3JpZ2luSW5zdGFuY2VJZCI6IjExMDVkMWQ0LTVmYWQtNDcwOS05MTE1LWU5ZjNlZDA4MGNjMyIsImFpZCI6IjNlMzBlYTkzLWUzN2EtNGJhZi1iY2NjLTM0ZDA3MGM2YjE0MCIsImJpVG9rZW4iOiI4ZDgxNmVhZS1jNTVhLTA0OTItMzk1YS05N2NhZWVmMWMxYmMiLCJzaXRlT3duZXJJZCI6IjI5OGY3NmM3LTExOTEtNDkzNy1hZjVhLTAyMmI0YjQxM2FhMiJ9&locale=en&pageId=nyc5u&section-url=https%3A%2F%2Fwww.lifewithdata.com%2Fblog-1%3Futm_medium%3DCommunity%26utm_source%3DDataCamp.com%2F&target=_top&viewMode=site&width=980#)

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' width='19' viewBox='0 0 19 19' role='img' class='blog-icon-fill blog-hover-container-element-fill js-evernote-checked' data-evernote-id='484'%3e%3cpath d='M2.44398805%2c5.99973295 C1.62345525%2c5.9690612 0.980075653%2c5.28418875 1.00047182%2c4.46312144 C1.02086799%2c3.64205413 1.69745853%2c2.98998831 2.51850166%2c3.0001164 C3.33954478%2c3.01024449 3.99985313%2c3.67880182 4%2c4.50012255 C3.98424812%2c5.34399206 3.28763905%2c6.0153508 2.44398805%2c5.99973295 L2.44398805%2c5.99973295 Z M2.44398805%2c10.9997329 C1.62345525%2c10.9690612 0.980075653%2c10.2841888 1.00047182%2c9.46312144 C1.02086799%2c8.64205413 1.69745853%2c7.98998831 2.51850166%2c8.0001164 C3.33954478%2c8.01024449 3.99985313%2c8.67880182 4%2c9.50012255 C3.98424812%2c10.3439921 3.28763905%2c11.0153508 2.44398805%2c10.9997329 L2.44398805%2c10.9997329 Z M2.44398805%2c15.9997329 C1.62345525%2c15.9690612 0.980075653%2c15.2841888 1.00047182%2c14.4631214 C1.02086799%2c13.6420541 1.69745853%2c12.9899883 2.51850166%2c13.0001164 C3.33954478%2c13.0102445 3.99985313%2c13.6788018 4%2c14.5001225 C3.98424812%2c15.3439921 3.28763905%2c16.0153508 2.44398805%2c15.9997329 L2.44398805%2c15.9997329 Z' data-evernote-id='485' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

# Statistics with Python-Frequency Distributions in Statistics.

![file.png](../_resources/aa32b5f614d73be463cafa1f128a46d6.png)[298f76_24e2f8f5c5f945018d21cf772bc49976~mv2.webp](../_resources/30f63111ddede3a6deea06c1e441b421.webp)

### In this post, you will learn -

1. What is Frequency Distribution?
2. How to create it?
3. Sorting frequency distribution table.
4. Sorting tables for Ordinal variables.
5. Proportions and Percentages.
6. Percentiles and Percentile ranks.
7. Grouped frequency distribution tables.

## 1. What is Frequency Distributions in Statistics?

![file.png](../_resources/839eca220e0f85738c5a7f6e217b675b.png)[062430dbfeba4663a6bf9465b05dee18.webp](../_resources/abecdd1c485c431638f6561d896385a8.webp)

Our ability to understand a data set just by looking at it in a table format is limited and it decreases dramatically as the size of the data set increases.

The above fig. is just a subset of data about the players in a [odi cricket matches](https://www.dropbox.com/s/9adk7j03fwrqbd5/odi_players_batting.csv?dl=0). This data set contains 1824 rows and 18 columns( not all shown). It would be very difficult to find any pattern in the data just by looking at it. Sometimes a data set can have more the million rows of data. So it's totally impossible to get a sense of it. To analyze a data set, we need to find a way to simplify it.

One way to simplify a data set is to select a variable and count how many times each unique value occurs and represent the frequencies( the number of times a unique value occurs in a table.)

The first column in the above figure tells from which country a player is ? and the second column tells us how many players are from each country.

We can see that In the first row, 237 players are from England, so the frequency of england is 237. The frequency of Australia is 205.

The 1824 rows of data are gets represented in just 12 rows. Now we can answers varies questions, like how many players are from each country? which country has maximum number of players and which has minimum? We can compare between two countries like England has almost twice the amount of players than the South Africa in this data set.

Without summarising this, it would be totally impossible to answers these kinds of questions.

Because the table above shows how frequencies are distributed, it is often called** frequency distribution table** or **frequency table** or **frequency distribution**.

## 2. How to create a Frequency Distribution table?

Let's first read the ODI players[cricket matches data set](https://www.dropbox.com/s/9adk7j03fwrqbd5/odi_players_batting.csv?dl=0).

To create a frequency distribution of the Country column, **Series.value_counts( )**  [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) of pandas.

It is as simple as that.

## 3. Sorting Frequency Distribution Tables -

By default, Pandas sorts the tables in the descending order of the frequencies.

In this case it is actually helpful. We can quickly understand which unique values have the greatest or lowest frequencies. We can make comparison easily.

But in other cases this might be difficult to analyze.

To sort the table by index in ascending order, we use the **Series.sort_index( ) **[method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sort_index.html) along with value_counts( ) method.

And to sort the table by index in a descending order, we pass the **ascending = False** in the sort_index( ) method.

## 4. Sorting tables for ordinal variables -

In ordinal scale, the measurement is done using words, like this below pic.

![file.png](../_resources/9bc703d27d61402810f5dc1ea109c99e.png)[298f76_9d816205251942eabc5defa1ae1e2cde~mv2.webp](../_resources/e7a4b355c8754b77eaaf1d3195d4b278.webp)

We can not use the **sort_index( )** method to sort in ascending or descending order.

Our data set doesn't have an variable that is measured in ordinal scale, so first let's create one using the Career Start**  **Column.

First let's see the maximum and minimum values of it.

We can see that maximum 2019 and minimum is 1971. Let's write a function that divide the career start values into 4 parts for the ordinal scale.

You can use the bins=4 parameter of value_counts( ) to come up with a binning strategy.

Now, we will create a new column for the ordinal scale in our odi dataframe using ** apply( )** method.

We can see that it has been successfully created.

Next let's create a frequency distribution table and try to sort it.

You can see that pandas couldn't able to sort the frequency table in ascending or descending order because it can't able to infer quantities from words. It can only sort them alphabetically.

The solution is to do selection by index lable or index position. Because the output of the odi['Career Start Ordinal].value_counts( ) is a series object with labels as indices.

To reorder in ascending order using labels we have to write -

We get the result we are looking for. But you can see that we have to type a lot when we reorder using labels. It would be better if we use index positions.

Let's do this -

We can see that most players started their career between 1995 to 2007.

## 5. Proportions and Percentages -

Let's create an another ordinal column for the **Batting Avg** variable. The Batting Avg represents how many runs, on average a batsman(player) scores before getting out.

When we analyze distributions we are often interested in answering questions about proportions and percentages.

Like -
1. What proportion of players have a very low avg?
2. What percentage of players have a good avg?

It would be hard to answer these types of questions precisely just by looking at the distribution.

If I want to find out what proportion of players have a very low avg. To do that I will divide the number of player who have very low avg by total number of players which will be 1315/ 1824. But it is much common to express fractions in decimals from 0 to 1.

So the answer will be 0.72 players have a very low avg.

In Pandas, we can compute all the proportions at once by dividing each frequencies by the total number of players or just use the **normalize** parameter of value_counts( ) set to True.

To find the percentages, we have to just multiply the proportions by 100.

## 6. Percentiles and Percentile ranks -

**What is percentile?**

A Percentile ( or a centile) is a measure in statistics indicating the value below which a given percentage of observations in a group of observations falls.

For example, the 20th percentile is the value( or score) below which 20% of the observation may be found.

To find percentiles, we can use the Series.describe( ) [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.describe.html), which returns by default the 25th, the 50th and the 75th percentiles.

At this moment we are not interested in the first three observations( count, mean and std), so we will use **iloc[ ]** to get the output we want.

The 25th, 50th, and 75th percentiles pandas returns by default are the scores that divide the distribution into four equal parts.

![file.png](../_resources/b0f18fae58389e9295184efac0dfa2cd.png)[48a2a42b19814efaa824450f23e8a253.webp](../_resources/cfe858739484828c9d8786113182fc2a.webp)

The three percentiles that divide the distribution in ***four*** equal parts are also known as **quartiles.**

1. The first quartile is 32 which means 25% of the runs are less than or equal to 32 runs.

2. The second quartile is 141 which means 50% of the runs are less than or equal to 141.

3. The third quartile is 672 which means 75% of the runs are less than or equal to 672.

These Percentages are also called the** percentile rank** which means 32 is also the 25th percentile. 141 is also the 50th percentile, 672 is also the 75th percentile.

You may want to find out other quartiles other than these 3. For that, we can use the **percentiles** parameter of **Series.describe( ). **This parameter requires us to pass the percentages we want as proportions between 0 and 1.

When we say what percentage of players have less than or equal to 672 runs? we are actually trying to find out the percentile rank. We know that the answer is 75%.

Suppose I want to know what percentage of players have less than or equal to 5000 runs or what percentage of players have greater than or equal to 10000 runs?

We can find these percentages more faster by using the **percentileofscore( a, score, kind='weak') **[function](https://docs.scipy.org/doc/scipy-0.10.0/reference/generated/scipy.stats.percentileofscore.html#scipy-stats-percentileofscore) from scipy.stats.

To get to our first question, we will write -

So 95.33 % of players have less than or equal to 5000 runs.

And to get the answer for the second question, we will subtract the value we will get from the percentileofscore function from 100.

The answer is 0.66 % of the players in our data set has more than or equal to 10000 runs.

## 7. Grouped Frequency Distribution Tables -

We create frequency distribution tables to transform relatively large and incomprehensible amounts of data to a table format we can understand. But sometimes we did get the result we expected.

Let's see what will happen when we apply the same technique that we use before.

We can see that there are way to many unique values, so grouping is not so perfect as we seen before.

One way to solve this is to use the **bins** parameter to group the values into equal intervals. Let's group the result into 10 bins.

The **(-18.427, 1842.6]** , **(1842.6, 3685.2]** are number intervals. The ( character indicates that the starting point is not included, while the ] indicates that the endpoint is included. **( -18.427, 184.6]** means -18.427 is not included in the interval but 184.6 is included.

Because we group values in a table to get a better sense of frequencies in the distribution, the table we generated above is also known as a **grouped frequency distribution table. **And** e**ach group (interval) in a grouped frequency distribution table is also known as a **class interval**.

If you look at the above grouped frequency table, you can see that it's not so neat, intervals are kind of messy and it is little bit hard to interpret the result.

To fix that we will use the Pandas **pd.interval_range( )**  [function](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.interval_range.html).

We will use 10 intervals, each consisting of 2000 runs.

Next, we create a new Series using the intervals as indices and for now, 0 as values.

1. Next we loop through all the values in the Runs Scored Column and for each value -

2. We loop through all the intervals that we defined and for each interval -

3. we check whether the current value of Runs Scored column belongs to that interval.

4. If the value doesn't belong to an interval, we continue the inner loop over the intervals.

5. If the value belongs to an interval - We update the counting for that interval in gr_freq_table by adding 1.

We exit the inner loop over the intervals with break because a value can belong to one interval only, and it makes no sense to continue the loop (without using break, we'll get the same output but we'll do many redundant iterations).

We can also check if we got it right or not by summing the gr_freq_table. There are 1824 rows of data so the frequencies should add up to 1824.

You can see that we have lost some data. May be changing the way we group it can solve the problem or if you know a better solution then please let me know in the comment section.

That's all for today. In the next post, we will learn how to visualize Frequency distributions. If you are not familiar with data visualization with matplotlib then please read this post - [Matplotlib Tutorial - data visualization with python.](https://www.lifewithdata.com/blog-1/matplotlib-tutorial-learn-data-visualization-with-python-in-brief)

If you find this post helpful then please subscribe below.

- [Statistics](https://www.lifewithdata.com/blog-1/categories/statistics)
- •
- [Data Science](https://www.lifewithdata.com/blog-1/categories/data-science)

### Recent Posts

[See All](https://www.lifewithdata.com/blog-1/)

[(L)](https://www.lifewithdata.com/blog-1/what-are-sampling-and-types-of-sampling-in-statistics)

#### [What are Sampling and types of sampling in Statistics?](https://www.lifewithdata.com/blog-1/what-are-sampling-and-types-of-sampling-in-statistics)

[(L)](https://www.lifewithdata.com/blog-1/statistics-with-python-visualizing-frequency-distributions)

#### [Statistics with Python - Visualizing Frequency Distributions.](https://www.lifewithdata.com/blog-1/statistics-with-python-visualizing-frequency-distributions)

[(L)](https://www.lifewithdata.com/blog-1/statistics-with-python-what-is-mean-in-statistics)

#### [Statistics with Python - What is Mean in Statistics?](https://www.lifewithdata.com/blog-1/statistics-with-python-what-is-mean-in-statistics)

Log in to leave a comment!

Join our mailing list
Never miss an update

- [

[298f76_0a1fa963b19b49e88740fbe8fed497bd~mv2.webp](../_resources/25b3ea9e9faba9a9337940aaf71cb6a9.webp)](https://www.linkedin.com/in/bhola-prasad-0065834b/)

- [

[e316f544f9094143b9eac01f1f19e697.webp](../_resources/86f7b9ec951c214f12a40d72bcb3a040.webp)](http://feeds.feedburner.com/LifeWithData)

COPYRIGHT © 2018 Life With Data

[ ![](data:image/svg+xml,%3csvg viewBox='0 0 569 546' version='1.1' xmlns='http://www.w3.org/2000/svg' data-evernote-id='7' class='js-evernote-checked'%3e%3cg%3e%3ccircle data-color='1' id='Oval' cx='362.589996' cy='204.589996' r='204.589996' data-evernote-id='8' class='js-evernote-checked'%3e%3c/circle%3e%3crect data-color='2' id='Rectangle' x='0' y='0' width='100' height='545.799988' data-evernote-id='9' class='js-evernote-checked'%3e%3c/rect%3e%3c/g%3e%3c/svg%3e)  Become a patron](https://www.patreon.com/bePatron?u=12171782&redirect_uri=https%3A%2F%2Fwww.lifewithdata.com&utm_medium=widget&utm_source=wix)

OK